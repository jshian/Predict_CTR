{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "df = pd.read_csv('./dataset/Apply_Rate_2019.csv').drop(['class_id'], axis=1)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df['apply'].unique(), y=df['apply'].value_counts())])\n",
    "fig.update_layout(xaxis_type='category', title_text='Apply frequency', title_x=0.5)\n",
    "fig.show()  # imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fking preprocessing\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# fill missing values by MICE, then change city_match back to 0/1\n",
    "# because directly assign 0 to city_match will worsen the result.\n",
    "# df.iloc[:,:6] = IterativeImputer().fit_transform(df.iloc[:,:6])\n",
    "df.iloc[:,:2] = IterativeImputer().fit_transform(df.iloc[:,:2])\n",
    "df.iloc[:,5:6] = IterativeImputer().fit_transform(df.iloc[:,5:6])\n",
    "df['city_match'] = df['city_match'].round()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of NaN in title_proximity_tfidf:',len(df[df['title_proximity_tfidf'].isnull()]))\n",
    "print('number of NaN in description_proximity_tfidf:',len(df[df['description_proximity_tfidf'].isnull()]))\n",
    "print('number of NaN in city_match:',len(df[df['city_match'].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = df[df['search_date_pacific']=='2018-01-27'].drop(['search_date_pacific'], axis=1)\n",
    "x = len(test_x)\n",
    "train_x = df[df['search_date_pacific']!='2018-01-27'].drop(['search_date_pacific'], axis=1)\n",
    "y = len(train_x)\n",
    "print('before separated:',len(df))\n",
    "print('after separated:',x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# train_x, test_x = train_test_split(df, test_size=0.2, random_state=4211)\n",
    "train_y = train_x.pop('apply')\n",
    "test_y = test_x.pop('apply')\n",
    "\n",
    "# Normalisation using the sklearn StandardScaler. Set the mean to 0 and sd to 1.\n",
    "transform_columns = ['title_proximity_tfidf', 'description_proximity_tfidf', 'main_query_tfidf', 'query_jl_score', 'query_title_score', 'job_age_days']\n",
    "ct = ColumnTransformer(\n",
    "        remainder='passthrough',\n",
    "        transformers=[('std', StandardScaler(), transform_columns)])\n",
    "train_x = ct.fit_transform(train_x)\n",
    "test_x = ct.fit_transform(test_x)\n",
    "\n",
    "# after ColumnTransformm, city_match is at the ENDDDDDDD of the dataframe, this took me hours to figure out :)\n",
    "x_columns = ['title_proximity_tfidf', 'description_proximity_tfidf', 'main_query_tfidf', 'query_jl_score', 'query_title_score', 'job_age_days', 'city_match']\n",
    "train_x = pd.DataFrame(train_x, columns=x_columns)\n",
    "test_x = pd.DataFrame(test_x, columns=x_columns)\n",
    "\n",
    "# undersampling to reduce imbalance\n",
    "#train_x, train_y = SMOTE(sampling_strategy=0.2, random_state=0).fit_resample(train_x, train_y)  # not much different\n",
    "train_x, train_y = RandomUnderSampler(random_state=0).fit_resample(train_x, train_y)\n",
    "\n",
    "train_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf):\n",
    "    pred_y = clf.predict(test_x)\n",
    "\n",
    "    print(classification_report(test_y, pred_y))\n",
    "    disp = plot_confusion_matrix(clf, test_x, test_y)\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(80, 60, ), learning_rate='adaptive', early_stopping=True, random_state=4211)\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(train_x, train_y)\n",
    "print(time.time()-start)\n",
    "report(mlp) # city_match not normalised, undersampling only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV  # this thing cpu intensive.\n",
    "tuned_parameters = {\n",
    "    'hidden_layer_sizes': [(120,), (80, 60,), (60, 40, 20,)]\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate': ['invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.002, 0.005],\n",
    "    'momentum', [0.9, 0.8]\n",
    "    'early_stopping': [True],\n",
    "    'random_state': [4211]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(), tuned_parameters, n_jobs=-1, verbose=2)\n",
    "clf.fit(train_x, train_y)\n",
    "report(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(train_x, train_y)\n",
    "print(time.time()-start)\n",
    "report(clf)  # true negative more but true positive less than above nn, lul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV  # this thing cpu intensive.\n",
    "tuned_parameters = {'C':np.arange(0.01,100,50)}\n",
    "\n",
    "clf = GridSearchCV(LinearSVC(), tuned_parameters, n_jobs=-1, verbose=2)\n",
    "clf.fit(train_x, train_y)\n",
    "report(clf)  # no much difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}